{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq,Attention\n",
    "## 参考资料\n",
    "#### 课件\n",
    "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
    "\n",
    "\n",
    "#### 论文\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "\n",
    "#### PyTorch代码\n",
    "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
    "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)\n",
    "\n",
    "\n",
    "#### 更多关于Machine Translation\n",
    "- [Beam Search](https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ)\n",
    "- Pointer network 文本摘要\n",
    "- Copy Mechanism 文本摘要\n",
    "- Converage Loss \n",
    "- ConvSeq2Seq\n",
    "- Transformer\n",
    "- Tensor2Tensor\n",
    "\n",
    "#### TODO\n",
    "- 建议尝试对中文进行分词\n",
    "\n",
    "#### NER\n",
    "- https://github.com/allenai/allennlp/tree/master/allennlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这份notebook当中，复现的是Luong的attention模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入中英文数据\n",
    "- 英文我们使用nltk的word tokenizer来分词，并且使用小写字母\n",
    "- 中文我们直接使用单个汉字作为基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    ''' 将数据加载进来 '''\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r',encoding=\"utf-8-sig\") as f: #encoding=\"utf-8-sig\" 处理中文，后边的 -sig 是为了去掉 ufeff\n",
    "        for line in f:\n",
    "            #print(line) #Anyone can do that.\t任何人都可以做到。\n",
    "            line = line.strip().split(\"\\t\") #分词后用逗号隔开\n",
    "            #print(line) #['Anyone can do that.', '任何人都可以做到。']\n",
    "            #en.append([\"BOS\"] + [c for c in line[0].lower().split()] + [\"EOS\"])\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            #BOS:beginning of sequence EOS:end of\n",
    "            #nltk.word_tolenize(): split English sentence\n",
    "            \n",
    "            # split Chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in jieba.cut(line[1])] + [\"EOS\"])\n",
    "           \n",
    "    return en, cn\n",
    "\n",
    "train_file = \"nmt/en-cn/train.txt\"\n",
    "dev_file = \"nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5491\n",
      "11264\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    ''' 构建字典 '''\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1  #word_count这里应该是个字典\n",
    "    ls = word_count.most_common(max_words) \n",
    "    #按每个单词数量排序前50000个,这个数字自己定的，不重复单词数没有50000\n",
    "    print(len(ls)) #train_en：5491\n",
    "    total_words = len(ls) + 2\n",
    "    #加的2是留给\"unk\"和\"pad\"\n",
    "    #ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    #加的2是留给\"unk\"和\"pad\",转换成字典格式。\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "#en_dict.items()把字典转换成可迭代对象，取出键值，并调换键值的位置。\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将预料数字化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    #en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....\n",
    "    \n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    #out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....\n",
    "    #.get(w, 0)，返回w对应的值，没有就为0.因题库比较小，这里所有的单词向量都有非零索引。\n",
    "    \n",
    " \n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "    \n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        \n",
    "        '''\n",
    "        根据句子长度对文本进行排序\n",
    "        #sorted()排序,key参数可以自定义规则，按seq[x]的长度排序，seq[0]为第一句话长度\n",
    "        '''\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "      \n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "    #print(sorted_index)\n",
    "    #sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....\n",
    "     #前面的索引都是最短句子的索引\n",
    "      \n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "     #print(out_en_sentences)\n",
    "     #out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......\n",
    "     \n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 今年 的 天氣 一直 異常 。 EOS\n",
      "BOS the weather has been unusual this year . EOS\n"
     ]
    }
   ],
   "source": [
    "k=9654\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将语料库分成 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size) # [0,15，30，45，60，75，90]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list) #打乱数据 【15，60，30，75，45，0，90】\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "        #所有batch放在一个大列表里\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "    '''\n",
    "    输入的是\n",
    "    sequence batch 【【2，4，1】，【1，46，7，23】，【31，4，752，23，74】...】\n",
    "    \n",
    "    输出的是 \n",
    "    长度一致的 sequence batch 【【2，4，1，0，0】，【1，46，7，23，0】，【31，4，752，23，74】...】\n",
    "    每个句子的长度\n",
    "    \n",
    "    '''\n",
    "#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........\n",
    "    lengths = [len(seq) for seq in seqs]#每个batch里语句的长度统计出来\n",
    "    n_samples = len(seqs) #一个batch有多少语句\n",
    "    max_len = np.max(lengths) #取出最长的的语句长度，后面用这个做padding基准\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    #先初始化全零矩阵，后面依次赋值\n",
    "    #print(x.shape) #64*最大句子长度\n",
    "    \n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    #print(x_lengths) \n",
    "    #这里看下面的输入语句发现英文句子长度都一样，中文句子长短不一。\n",
    "    #说明英文句子是特征，中文句子是标签。\n",
    "\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "      #取出一个batch的每条语句和对应的索引\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        #每条语句按行赋值给x，x会有一些零值没有被赋值。\n",
    "        \n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        #按打乱的batch序号分数据，打乱只是batch打乱，一个batach里面的语句还是顺序的。\n",
    "        #print(mb_en_sentences)\n",
    "        \n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        #返回的维度为：mb_x=(64 * 最大句子长度）,mb_x_len=最大句子长度\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        \n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "        #这里把所有batch数据集合到一起。\n",
    "        #依次为英文句子，英文长度，中文句子翻译，中文句子长度，这四个放在一个列表中\n",
    "        #一个列表为一个batch的数据，所有batch组成一个大列表数据\n",
    "\n",
    "        \n",
    "    return all_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 没有 Attention 的 seq2seq 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20191127162941278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2NzYxOTQ1,size_16,color_FFFFFF,t_70)\n",
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        #以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        #这里的hidden_size为embedding_dim：一个单词的维度 \n",
    "        #torch.nn.Embedding(num_embeddings, embedding_dim, .....)\n",
    "        #这里的hidden_size = 100\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)      \n",
    "        #第一个参数为input_size ：输入特征数量\n",
    "        #第二个参数为hidden_size ：隐藏层特征数量\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths): \n",
    "        #x是输入的batch的所有单词，lengths：batch里每个句子的长度\n",
    "        #因为需要把最后一个hidden state取出来，需要知道长度，因为句子长度不一样\n",
    "        ##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])\n",
    "        # lengths= =tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        ''' 把句子按照长度排序 '''\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        #按照长度排序，descending=True长的在前。\n",
    "        #返回两个参数，句子长度和未排序前的索引\n",
    "        # sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])\n",
    "        # sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        \n",
    "        x_sorted = x[sorted_idx.long()] #句子用新的idx，按长度排好序了\n",
    "        \n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        #print(embedded.shape)=torch.Size([64, 10, 100])\n",
    "        #tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....\n",
    "        \n",
    "        '''\n",
    "        因为句子的长度不一样，我们获取的最后的 hidden state 位置不一定是 rnn 的最后一个 hidden state \n",
    "        直接 将 embedded 进行 rnn 训练的话 默认是 返回的最后一个 hidden state\n",
    "        '''\n",
    "        ''' 去掉数据中的补 0 ，因为 function 要求数据要根据长度排序 进入处理 所以上面进行了数据的排序处理'''\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        #这个函数就是用来处理不同长度的句子的，https: // www.cnblogs.com / sbj123456789 / p / 9834018. html\n",
    "\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        ''' 将补 0 加回来'''\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        #out.shape = torch.Size([64, 10, 100]),\n",
    "        \n",
    "        ''' 恢复 batch 数据的原顺序 '''\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        ''' \n",
    "        因为之前的处理 数据在内存中可能不连续了，这里使用 contiguous() 来处理一下数据\n",
    "        contiguous() 就是将不连续的内存单元连续在一起\n",
    "        '''\n",
    "        #out.shape = torch.Size([64, 10, 100])\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        # 因为 hid 的【num_layers * num_direction, batch_size, hidden_size】\n",
    "        return out, hid[[-1]] #有时候num_layers层数多，或者是双向的 需要取出最后一层 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        #print(y.shape)=torch.Size([64, 12])\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        #中文的y和y_lengths\n",
    "        ''' 排序 '''\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()] #隐藏层也要排序\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        \n",
    "        ''' 因为encode的hidden state 传入每一层的输入所以这里要对输入做一个 concatnate 连接处理'''\n",
    "        #y_sorted = torch.cat([y_sorted,hid.ensqueeze(0).expand_as(y_sorted)],2) \n",
    "        #ensqueeze(0) 因为hidden state 只有一个要对每一个输入都添加所以这里对第0维进行扩展\n",
    "        #expand_as(y_sorted)扩展的维度就是 y_sorted 的维度\n",
    "        \n",
    "        # batch_size, output_length, embed_size\n",
    "        ''' pack 处理'''\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        ''' rnn 默认不传 hidden state 时 是0，这里我们需要把上encode的最后一层的 hidden state 传过来'''\n",
    "        out, newhid = self.rnn(packed_seq, hid) #加上隐藏层\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        \n",
    "        ''' 恢复 pad'''\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        ''' 恢复排序'''\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        \n",
    "        output_seq = unpacked[original_idx.long()].contiguous()#print(output_seq.shape)=torch.Size([64, 12, 100])\n",
    "        newhid = newhid[:, original_idx.long()].contiguous()#print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        ''' 对输出进行拼接'''\n",
    "        #output_seq = torch.cat([output_seq,hid.ensqueeze(0).expand_as(y_sorted)],2) \n",
    "        ''' 通过 FC 计算结果'''\n",
    "        output = F.log_softmax(self.out(output_seq), -1)#print(output.shape)=torch.Size([64, 12, 3195])\n",
    "        \n",
    "        return output, newhid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PlainSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        #encoder是上面PlainEncoder的实例\n",
    "        #decoder是上面PlainDecoder的实例\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "       \n",
    "    #把两个模型串起来 \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        #self.encoder(x, x_lengths)调用PlainEncoder里面forward的方法\n",
    "        #返回forward的out和hid\n",
    "        \n",
    "        output, hid = self.decoder(y=y,y_lengths=y_lengths,hid=hid)\n",
    "        #self.dencoder()调用PlainDecoder里面forward的方法\n",
    "        \n",
    "        return output, None\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        #x是一个句子，用数值表示\n",
    "        #y是句子的长度\n",
    "        #y是“bos”的数值索引=2\n",
    "        \n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid) \n",
    "            \n",
    "#刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LanguageModelCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        #target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....\n",
    "        #  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....\n",
    "        #print(input.shape,target.shape,mask.shape)\n",
    "        #torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])\n",
    "        \n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        \n",
    "        # target: batch_size * 1=768*1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        #print(-input.gather(1, target))\n",
    "        output = -input.gather(1, target) * mask\n",
    "        #这里算得就是交叉熵损失，前面已经算了F.log_softmax\n",
    "        #.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038\n",
    "        #output.shape=torch.Size([768, 1])\n",
    "        #mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了\n",
    "        \n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        #均值损失\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            #（英文batch，英文长度，中文batch，中文长度）\n",
    "            \n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            \n",
    "            #前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            #\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            #输入输出的长度都减一。\n",
    "            \n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            #返回的是类PlainSeq2Seq里forward函数的两个返回值\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            #mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],\n",
    "            #mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1\n",
    "            #mb_out_mask就是LanguageModelCriterion的传入参数mask。\n",
    "\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            #一个batch里多少个单词\n",
    "            \n",
    "            total_loss += loss.item() * num_words\n",
    "            #总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数\n",
    "            \n",
    "            total_num_words += num_words\n",
    "            #总单词数\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            ''' RNN 防止梯度爆炸 要进行 clip grad'''\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            #为了防止梯度过大，设置梯度的阈值\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data) #评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "\n",
    "#传入中文和英文参数\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 9.347283363342285\n",
      "Epoch 0 iteration 100 loss 4.824400424957275\n",
      "Epoch 0 iteration 200 loss 5.507288932800293\n",
      "Epoch 0 Training loss 5.870748127984871\n",
      "Evaluation loss 5.188047265914972\n",
      "Epoch 1 iteration 0 loss 5.206871032714844\n",
      "Epoch 1 iteration 100 loss 4.175070762634277\n",
      "Epoch 1 iteration 200 loss 5.044918060302734\n",
      "Epoch 1 Training loss 4.945426733946697\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻译测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮膚 真好 。 EOS\n",
      "你的人。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 UNK 正确 。 EOS\n",
      "你的人。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每個 人 都 佩服 他 的 勇氣 。 EOS\n",
      "他是他的。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几点 了 ？ EOS\n",
      "他是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今晚 有空 。 EOS\n",
      "我不知道。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這是 你 的 書 。 EOS\n",
      "你的。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他们 在 吃 午饭 。 EOS\n",
      "我在这里。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這把 椅子 UNK 。 EOS\n",
      "他是我的。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 UNK 。 EOS\n",
      "他是我的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很多 人 都 参加 了 他 的 UNK 。 EOS\n",
      "他是他的。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训练 。 EOS\n",
      "她的人。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有人 在 看 著 你 。 EOS\n",
      "你的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我不。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜歡 流行 音樂 。 EOS\n",
      "我不知道。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS Tom 沒有 孩子 。 EOS\n",
      "汤姆是我的。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 UNK 上 。 EOS\n",
      "我的。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤姆 冷静下来 了 。 EOS\n",
      "汤姆是我的。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大聲 一點兒 。 EOS\n",
      "我在他的。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周日 空 出来 。 EOS\n",
      "他是我的。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS UNK 了 一個 錯 。 EOS\n",
      "我不知道。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#翻译个句子看看结果咋样\n",
    "def translate_dev(i):\n",
    "    #随便取出句子\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    #把句子升维，并转换成tensor\n",
    "    \n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    #取出句子长度，并转换成tensor\n",
    "    \n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    #bos=tensor([[2]])\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    #这里传入bos作为首个单词的输入\n",
    "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])\n",
    "    \n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\": # 把数值变成单词形式\n",
    "            trans.append(word) #\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
