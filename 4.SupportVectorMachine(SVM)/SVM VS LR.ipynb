{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转载：\n",
    "## LR 和 SVM 的区别与联系\n",
    "### 联系\n",
    "#### 都是分类算法\n",
    "\n",
    "在很大一部分人眼里，LR是回归算法。我是非常不赞同这一点的，因为我认为判断一个算法是分类还是回归算法的唯一标准就是样本label的类型，如果label是离散的，就是分类算法，如果label是连续的，就是回归算法。很明显，LR的训练数据的label是“0或者1”，当然是分类算法。其实这样不重要啦，暂且迁就我认为他是分类算法吧，再说了，SVM也可以回归用呢。\n",
    "\n",
    "#### 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。\n",
    "\n",
    "这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？\n",
    "\n",
    "#### LR和SVM都是监督学习算法。\n",
    "\n",
    "#### LR和SVM都是判别模型。\n",
    "判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别（哈哈，废话是不是太多）。\n",
    "\n",
    "### 不同点\n",
    "#### loss function 不同 (重点)\n",
    "\n",
    "##### LR 的 loss function\n",
    "J(θ)=−1m(∑i=1my(i)logy^(i)(x(i))+(1−y(i))log(1−y^(i)))\n",
    "J(θ)=−1m(∑i=1my(i)logy^(i)(x(i))+(1−y(i))log(1−y^(i)))\n",
    "##### SVM 的 loss function\n",
    "L(w,b,α)=12∥w∥2−∑i=1nαi(yi(wTxi+b)−1)\n",
    "L(w,b,α)=12‖w‖2−∑i=1nαi(yi(wTxi+b)−1)\n",
    "<br>SVM 只考虑局部的边界线附近的点，LR 考虑全局，远离的点对边界线的确定也起作用\n",
    "\n",
    "线性 SVM 不直接依赖于数据分布，分类平面不受一类点的影响。\n",
    "LR 则受所有数据点的影响, 如果数据不同类别 strongly unbalance， 一般需要先对数据做 balancing。\n",
    "\n",
    "在解决非线性问题时，SVM 采用核函数的机制，而 LR 通常不采用核函数的方法\n",
    "\n",
    "在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​\n",
    "\n",
    "线性 SVM 依赖数据表达的距离测度，所以需要先对数据做 normalization, LR 则不受影响。\n",
    "\n",
    "SVM 的损失函数就自带正则，即12∥w∥212‖w‖2, 这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！\n",
    "\n",
    "### LR 和 SVM 的选择\n",
    "在Andrew NG的课里讲到过：\n",
    "\n",
    "如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\n",
    "如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel\n",
    "如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况\n",
    "### SVM 的 kernel 一般怎么选择?\n",
    "如果数据量不是特别大的时候，可以使用 RBF kernel，主要要加入正则化；\n",
    "如果数据量很大，需要把特征离散化，拉高维度，使用 linear kernel 的 SVM\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「David_Hernandez」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/kisslotus/article/details/80106191\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
