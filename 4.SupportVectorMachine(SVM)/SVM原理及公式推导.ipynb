{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机即 Support Vector Machine，简称 SVM 。SVM模型的主要思想是在样本特征空间上找到最佳的分离超平面（二维是线）使得训练集上正负样本间隔最大，这个约束使得在感知机的基础上保证可以找到一个最好的分割分离超平面（也就是说感知机会有多个解）。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。\n",
    "一般SVM有下面三种：\n",
    "<br>一、线性可分支持向量机\n",
    "<br>硬间隔支持向量机：硬间隔最大化（hard margin maximization），当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。说的白话一点就是两团数据分的很开，没有你中有我我中有你的情况，画一条线完全可以将数据分开，在数据点不多的情况下我们可以直接通过解析解求得w和b。\n",
    "![](https://img-blog.csdnimg.cn/20181129201800255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM4MzU1OA==,size_16,color_FFFFFF,t_70)\n",
    "<br>二、线性支持向量机\n",
    "<br>软间隔支持向量机：软间隔最大化（soft margin maximization ），当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。两团数据分的很开，存在少量样本点轻微越界的情况，我们可以容忍少部分样本分错，保证得到的模型鲁棒性最高，泛化能力更强。\n",
    "![](https://img-blog.csdnimg.cn/20181130110054247.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM4MzU1OA==,size_16,color_FFFFFF,t_70)\n",
    "<br>三、非线性支持向量机\n",
    "<br>当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。\n",
    "![](https://img-blog.csdnimg.cn/2018112920274235.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM4MzU1OA==,size_16,color_FFFFFF,t_70)\n",
    "<br>SVM 一直被认为是效果最好的现成可用的分类算法之一，即使是现在也常应用在图像识别、文本识别、生物科学和其他科学领域，在金融量化分类中一直是较为理想的，稳定的分类模型。“现成可用”其实是很重要的，因为一直以来学术界和工业界甚至只是学术界里做理论的和做应用的之间，都有一种“鸿沟”，有些很精妙或者很复杂的算法，在抽象出来的模型里很完美，然而在实际问题上却显得很脆弱，效果很差甚至全军覆没。而 SVM 则非常稳定，理论严谨，可以线性分类，也可以高维度非线性分类，工业应用稳定可靠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从线性分类器开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性可分的数据集，可能有很多种可选择的分类直线，SVM 可以选择最合适的一条直线。\n",
    "![](https://img-blog.csdn.net/20180521035815684)\n",
    "<br>假设分类的超平面的方程为 $WX+B=0$\n",
    "<br>支撑向量分别是 $WX+B=1$$WX+B=-1$ ,其中的 1 和 -1 是为了方便计算\n",
    "<br>我们希望 分类边界 之间的距离尽可能的大。这时需要计算两个支撑向量的超平面之间的距离，这时我们就需要用到计算点与平面之间距离的公式：\n",
    "$$d = \\frac{w_1 x_1+w_2 x_2+...+w_n x_n+b}{\\sqrt{w_1^2+w_2^2+...+w_n^2}} =\\frac{WX+b}{\\sqrt{W^2}} $$\n",
    "<br>带入 支撑向量的公式，可以得到两个支撑向量之间的距离$d = \\frac{2}{||W||}$\n",
    "<br>这时我们的目标就是 $maximize = \\frac{2}{||W||}$ \n",
    "<br>为了方便计算转换为 $minimize = \\frac{1}{2}||W||$ \n",
    "<br>等价于 $argmin = \\frac{1}{2}||W||^2$ \n",
    "<br>我们的目标函数是有限制的限制条件是：\n",
    "$$  Y_i（W^TX_i+B)\\geq 1$$\n",
    "<br>这时的目标函数有一个缺点，要求样本不能落在支撑向量的区间内，对样本的噪声容忍度小，容易过拟合\n",
    "<br>这时我们可以引入惩罚项 $\\varepsilon$ 此时的目标公式和约束条件分别是 $$argmin = \\frac{1}{2}||W||^2+\\lambda \\sum_{i=1}^{n}\\varepsilon_i$$ \n",
    "$$ Y_i（W^TX_i+B)\\geq 1-\\varepsilon$$ \n",
    "<br>因为目标函数要取最小值 所以 $\\varepsilon \\geq 1-Y_i（W^TX_i+B)$ 就取 $\\varepsilon = 1-Y_i（W^TX_i+B)$  \n",
    "<br>又因为作为惩罚项 $\\varepsilon  \\geq  0$ \n",
    "<br>所以最终的目标函数是 $$minimize L= \\frac{1}{2}||W||^2+\\lambda \\sum_{i=1}^{n} max\\left(0,1-Y_i(W^TX_i+B)\\right)$$ \n",
    "<br> 也可以写成$$  L= \\frac{1}{2}||W||^2+ \\sum_{i=1}^{n}\\lambda_i [1-Y_i(W^TX_i+B)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUAL 对偶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 目标函数$  L= \\frac{1}{2}||W||^2+\\lambda_i \\sum_{i=1}^{n}\\lambda_i [1-Y_i(W^TX_i+B)] $的 KKT 条件是 \n",
    "<br>$\\lambda_i <br> 0$\n",
    "<br>$1-Y_i(W^TX_i+b)\\leq0 $ \n",
    "<br>$\\lambda_i []=0$\n",
    "<br>L对w的偏导数为0时得到 $w = \\sum_{i=1}^{n}\\lambda_i y_i x_i$ L对b的偏导数为0 将其带入到 L 公式我们得到了 L的对偶函数：\n",
    "$$ L = -\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n}\\lambda_i\\lambda_j y_i y_j x_i^T x_j + \\sum_{i=1}^{n}\\lambda_i$$\n",
    "<br>其中的 $ x_i^T x_j $ 就是需要计算的项，也是核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "总体过程就是，先推导出了SVM的目标函数，为了简便计算，我们对目标函数的对偶函数进行优化得到次优解，对偶函数实际上就是将数据映射到了高维空间，这时可以使用核函数减少计算量\n",
    "#### 目标函数\n",
    "$$ minimize_{w,b}\\frac{1}{2}||W||^2+ \\sum_{i=1}^{n}\\lambda_i [1-Y_i(W^TX_i+B)]  $$\n",
    "#### 对偶形式\n",
    "$$ minimize -\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n}\\lambda_i\\lambda_j y_i y_j x_i^T x_j + \\sum_{i=1}^{n}\\lambda_i $$\n",
    "#### 核方法的引出\n",
    "对偶实际上是将数据映射到了高维空间\n",
    "$$ minimize -\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n}\\lambda_i\\lambda_j y_i y_j \\phi(x_i^T)\\phi( x_j) + \\sum_{i=1}^{n}\\lambda_i $$\n",
    "我们可以使用核方法来减少计算量\n",
    "$$K(x_i,x_j)=< \\phi(x_i^T),\\phi( x_j)>= \\phi(x_i^T)\\phi( x_j)$$\n",
    "#### 常见的核方法\n",
    "线性核：$K(x_i,x_j) = x_i^T x_j$\n",
    "<br>多项式核：$K(x_i,x_j) = (\\gamma x_i^T x_j+\\gamma)^d$\n",
    "<br>高斯核: $K(x_i,x_j) = exp(-\\gamma ||x_i- x_j||^2);\\gamma > 0$\n",
    "<br>sigmoid核: $K(x_i,x_j) = tanh(\\gamma x_i^T x_j+\\gamma) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何选取核函数？\n",
    "多个核函数结合\n",
    "<br><br>特征数量大到和样本数量差不多。使用LR或者SVM + 线性核\n",
    "<br><br>特征数量小，样本数量正常。SVM + 高斯核\n",
    "<br><br>特征数量小，样本数量很大。需要手动添加一些特征，从而变成第一种情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "https://www.cnblogs.com/hanzi5/p/10105614.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
