{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "## 线性回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是使用数理统计中回归分析，来确定两种或两种以上变量之间相互依赖关系的一种统计方法。表达形式为$ y=wx+e $\n",
    "<br>一元线性回归分析： $y=wx+b$,只包含一个自变量和因变量，二者可以用一条直线近似的表示\n",
    "<br>多元线性回归分析： $h_\\theta(x1,x2,...xn)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$,包含两个或两个以上的自变量，并且因变量和自变量之间是线性关系，但自变量与自变量之间不一定是线性关系，或者说不一定有关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数定义为：$$loss=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})$$ 计算的是预测值与实际值距离的平均值。（欧式距离）\n",
    "<br>为什么使用均方差平方差作为损失函数？\n",
    "<br>我们希望线性回归的预测值与真实值之间的差距越小越好，线性回归的结果是连续值，值域是$(+\\infty,-\\infty)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "## 逻辑回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归是一种用于解决二分类问题的方法，用来通过对两种或两种以上的变量之间的依赖关系进行估计，得到因变量结果的可能性。这里的可能性并非是数学上的概率值，不可以直接当作概率值来用。\n",
    "<br>逻辑回归公式：$$h_\\theta(x)=\\frac{1}{1+e^{-\\theta^TX}}$$\n",
    "也可以看作是将线性函数$h_\\theta=\\theta^Tx$通过Sigmoid函数映射到了（0，1）之间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归公式推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的数据集$(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n),X_i=(x_1,x_2,...,x_m),Y_i=0 or 1$,对数据进行分类，我们希望在指定X的条件下Y的概率越大越好，即：$p(Y|X)$。通过公式我们可以看出逻辑回归是一个判别模型。\n",
    "<br><br>$p(Y|X)$是怎么做分类的？\n",
    "<br>通过比较$p(Y=1|X)$与$p(Y=0|X)$的大小来表示。\n",
    "<br><br>可不可用线性回归来表示$p(Y|X)$，即$p(Y|X)=\\theta^TX+b$?\n",
    "<br>不可以，$p(Y|X)$是条件概率值域在（0，1）之间，线性回归的值域是$(+\\infty,-\\infty)$.\n",
    "<br><br>所以我们使用逻辑函数（sigmoid）来进行表示 $h_\\theta(x)=\\frac{1}{1+e^{-\\theta^TX}}$\n",
    "<br>对于二分类:$$p(Y=1|X)=\\frac{1}{1+e^{-\\theta^TX}}$$ \n",
    "<br>$$p(Y=0|X)=\\frac{e^{-\\theta^TX}}{1+e^{-\\theta^TX}}$$\n",
    "总结在一起就是$$p(Y|X)=p(Y=1|X)^y[1-p(Y=1|X)]^{1-y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策边界 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑函数是否是线性分类器呢？\n",
    "我们可以通过决策边界来进行分析，决策边界是线性的可以认为是线性分类器，否则就是非线性分类器\n",
    "![决策边界图片](https://cn.bing.com/th?id=OIP.z3R7OKV4myihO65gjuI3twHaC8&pid=Api&rs=1)\n",
    "<br>决策边界的确定：\n",
    "<br>落在决策边界上的点$$\\frac{p(Y=1|X)}{p(Y==|X)}=1$$<br> $$ e^{-\\theta^TX}=1$$\n",
    "两边一起取log：\n",
    "$$log^{e^{-\\theta^Tx}}=log^{1}$$\n",
    "<br>\n",
    "$$-\\theta^Tx=0$$\n",
    "由此可见决策边界是线性的，所以逻辑函数是线性分类器。\n",
    "<br><br>在这里我们也可以看出逻辑函数之所以是线性的是因为它将线性函数 $h_\\theta=-\\theta^Tx$ 通过逻辑函数sigmoid映射到（0，1）中，如果被映射的函数是非线性函数的时候，逻辑函数是否就是非线性分类器呢？\n",
    "<br>使用 kernel，或者可以将神经网络的最后一层看作是LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数（Cost Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代价函数与损失函数的关系（Cost Fucntion vs Loss Function）\n",
    "<br>简单说，loss function是对于单个样本而言的，比如对于0-1分类问题，当前预测样本x的输出为t，实际值为y，那么loss function就是y-t，或者abs(y-t)；对于连续型数据的预测，也就是回归问题，loss function可以是差值的平方：(y-t)^2\n",
    "<br>而cost function是对于样本总体而言的，对于0-1分类问题，loss function是n个样本的loss function取值的均值；而对于回归问题，cost function是n个样本的平方误差的平均，俗称均方误差(mean square error)\n",
    "<br>总结：cost function是各个样本的loss funcion的平均\n",
    "<br>\n",
    "<br>逻辑回归使用的损失函数是：最大似然估计，交叉熵（Corss Entropy），不是最小二乘法，方差（Squared error）\n",
    "<br>因为使用最小二乘法计算损失函数，目标函数是：$E_\\theta=\\left(y_i-\\frac{1}{1+e^{-\\theta^TX}}\\right)$ 结果是非凸函数![非凸函数](https://pic4.zhimg.com/80/v2-50e67b8caa634bc77abf67ee075ada00_hd.jpg)\n",
    "<br>使用最大似然估计结果是:$\\theta_{MLE}=argmin_\\theta -\\sum_{i=1}^{n}\\left(y_i log^{p(Y=1|X_i;\\theta)}+(1-y_i) log^{(1-p(Y=1|X_i;\\theta))}\\right)$是凸函数![凸函数](https://pic4.zhimg.com/80/v2-5b13c52423931adc0f0e38beb1f8e8eb_hd.jpg)\n",
    "通过分析也可以知道，预测结果是在（0，1）之间的，所以预测值与真实值之间的距离会在（0，1）之间不断地震荡。\n",
    "<br>\n",
    "<br>所以我们采用最大似然函数来计算损失，或者说使用交叉熵来计算概率值之间的距离。\n",
    "<br>推导过程\n",
    "<br>首先，我们定义的函数是：$p(Y|X)=p(Y=1|X)^y[1-p(Y=1|X)]^{1-y}$\n",
    "<br>损失函数是：\n",
    "<br>$$\\begin{eqnarray*}\n",
    "\\theta_{MLE} \n",
    "&=& argmax_\\theta\\prod_{i=0}^np(y_i|x_i;\\theta)\\\\\n",
    "&=& argmax_\\theta log^{\\left(\\prod_{i=0}^np(Y_i|X_i;\\theta)\\right)}(引入log减少计算)\\\\\n",
    "&=& argmin_\\theta -\\sum_{i=1}^{n}log^{(p(Y_i|X_i;\\theta)}(引入 - 由求最大转为求最小)\\\\\n",
    "&=& argmin_\\theta -\\sum_{i=1}^{n}log^{(p(Y=1|X_i;\\theta)^y_i[1-p(Y=1|X_i;\\theta)]^{1-y_i})}\\\\\n",
    "&=& argmin_\\theta -\\sum_{i=1}^{n}\\left(y_i log^{p(Y=1|X_i;\\theta)}+(1-y_i) log^{(1-p(Y=1|X_i;\\theta))}\\right)\n",
    "\\end{eqnarray*}$$\n",
    "<br>由损失函数与代价函数之间的关系可以知道,代价函数(Cost Function)是：\n",
    "<br>$$\\begin{eqnarray*}\n",
    "J_\\theta &=&  \\frac{1}{n}loss\\\\\n",
    "&=& - \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i log^{p(Y=1|X_i;\\theta)}+(1-y_i) log^{(1-p(Y=1|X_i;\\theta))}\\right)\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(此处将 \\theta 写作 W 为了方便和程序对应)\n",
    "$$\\begin{eqnarray*}\n",
    "\\frac{\\partial loss(W)}{\\partial W} \n",
    "&=& \\frac{\\partial -\\sum_{i=1}^{n}\\left(y_i log^{p(Y=1|X_i;W)}+(1-y_i) log^{(1-p(Y=1|X_i;W))}\\right)}{\\partial W}\\\\\n",
    "&=& \\frac{\\partial -\\sum_{i=1}^{n}\\left(y_i log^{\\sigma(W^T X))}+(1-y_i) log^{(1-\\sigma(W^T X))}\\right)}{\\partial W}(此处将 p(Y=1|X_i;W)=\\frac{1}{1+e^{-\\theta^TX}} 换成 \\sigma(W^TX)是为了方便书写)\\\\\n",
    "&=& -\\sum_{i=1}^{n}\\left(y_i \\frac{\\sigma(W^T X)(1-\\sigma(W^T X))}{\\sigma(W^T X)} X_i + (1-y_i)\\frac{-\\sigma(W^T X)(1-\\sigma(W^T X))}{\\sigma(W^T X)}X_i  \\right)\\\\\n",
    "&=&  -\\sum_{i=1}^{n}\\left(y_i\\sigma(W^T X)X_i +（y_i-1)\\sigma(W^T X)X_i\\right)\\\\\n",
    "&=& \\sum_{i=1}^{n}(\\sigma(W^T X)-y_i)X_i\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "梯度的更新：$W^{t+1} =W^t - \\eta \\frac{\\partial loss(W)}{\\partial W} =W^t - \\eta \\sum_{i=1}^{n}(\\sigma(W^T X)-y_i)X_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降是每计算出一个样本就更新一次梯度。\n",
    "<br>\n",
    "<br>优点是权重更新的速度快\n",
    "<br>缺点是权重更新时会带来噪音\n",
    "\n",
    "<br>改善方式是将学习率设置的小一些\n",
    "<br>\n",
    "<br>随机梯度下降的梯度更新公式是：$$W^{t+1} =W^t - \\eta (\\sigma(W^T X)-y_i)X_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则项的引入 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当给定是数据线性可分是，逻辑回归的参数会趋向于无穷大。\n",
    "为了限制参数，在逻辑回归中引入了L2正则项\n",
    "$$W_{MLE} = argmin_W - \\prod_{i=0}^np(y_i|x_i;W) + \\lambda||W||_{2}^{2}$$\n",
    "公式中的 $\\lambda||W||_{2}^{2}$ 是L2正则项，其中的 $\\lambda$ 是超参数，当W使得公式结果越来越小时，引入W自身的值来控制，使得权重不能无限的增大。\n",
    "<br> $||W||_{2}^{2} = W_1^2+W_2^2+...+W_N^2$\n",
    "<br><br> 在计算梯度下降的导数时\n",
    "$$\\frac{\\partial loss(W)}{\\partial W} = \\sum_{i=1}^{n}(\\sigma(W^T X)-y_i)X_i + 2 \\lambda W $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 vs 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过一个实际的例子来进行比较：\n",
    "<br> 有一组数据，\n",
    "<br>\n",
    "|  姓名  | 年龄  |  工龄  | 工资  |\n",
    "|  ----  | ----  |  ----  | ----  |\n",
    "| 圣埃蒂 |  23   | 3    | 1234.4|\n",
    "| 阿斯顿 |  43   | 23   | 2454.2|\n",
    "<br><br>当给出一个新的样本的年龄和工龄，让我们来预测他的工资的时候，我们可以使用线性回归来计算，因为工资作为因变量是和年龄和工资有线性关系的，而且自变量和因变量都是连续值\n",
    "<br><br>但是当我们给出的数据的特征更多的时候，尤其是加入了非线性的特征，比如是否会逾期还款，线性回归就没办法进行计算了，因为是否预期特征值本身只有 0 ，1 两种情况，这种情况下就需要使用逻辑回归来计算出特征之间的相互依赖的关系，注意，此处的相互依赖的关系并没有线性要求，这种关系可以是线性的也可以是非线性的，使用逻辑回归只是计算出各个特征与预测结果在概率上的依赖程度，比如年龄23更偏向于逾期，工资2454.2，更偏向于不逾期。\n",
    "<br>\n",
    "<br>通过以上示例可以看出：\n",
    "<br>在结果上线性回归的结果是连续值，且值域是，$(+\\infty,-\\infty)$。\n",
    "<br>逻辑回归的结果是离散值。\n",
    "<br>\n",
    "<br>在特征上来看，线性回归适合特征指示连续值且特征值和预测值之间有线性关系的数据集。\n",
    "<br>逻辑回归更适合特征值维度比较多，且特征值之间不一定有线性关系。\n",
    "<br>\n",
    "<br>逻辑回归是一种广义的线性回归\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑斯特回归为什么要对特征进行离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性，逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；\n",
    "<br><br>速度快，稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；\n",
    "<br><br>鲁棒性,离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；\n",
    "<br><br>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；\n",
    "<br><br>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；\n",
    "<br><br>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归输出的值是真实的概率吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转载自简书：https://www.jianshu.com/p/a8d6b40da0cf\n",
    "逻辑回归作为被广泛使用的二分类模型，面试中自然是不可缺少的。但要深刻理解逻辑回归又不是那么容易的，比如说，逻辑回归输出的值是0到1之间的值，这个值是真实的概率吗？逻辑回归为什么要选择sigmoid函数的形式，而不是其他将数值映射到0到1之间的形式？本文试图给出一个尽可能简单明了的分析。\n",
    "<br><br>一、从一个例子开始\n",
    "<br><br>假设你在一家金融公司工作，老板交给你一个任务，建一个模型，用来预测一个借款人是否会违约，公司拥有一个借款人的特征数据，比如年龄。\n",
    "<br><br>\n",
    "将是否违约作为标签变量y，0表示没有违约，1表示违约。在给定特征x的情况下，我们假设 y 是一个服从伯努利分布的二值随机变量。注意，这是我们做的第一个假设哦！从某种意义上讲，模型准不准，首先要看假设合不合理。\n",
    "<br><br>\n",
    "我们的任务用数学语言描述就是，寻找一个模型，输入x后，可以告诉我们y所服从的随机分布的参数，知道参数后，就可以计算y的期望作为预测。\n",
    "<br><br>\n",
    "具体到违约预测，上面所说的随机分布就是指伯努利分布，该分布的参数就是Φ=P(y=1)，同时也是该分布的期望。\n",
    "<br><br>\n",
    "请认真体会一下我们的思路：\n",
    "<br><br>\n",
    "<br>1、对每一个确定的x，y仍然是一个随机变量\n",
    "<br>2、该随机变量服从某个随机分布\n",
    "<br>3、努力求出这个随机分布的参数\n",
    "<br>4、求出该随机分布的期望\n",
    "<br>5、将期望作为预测值\n",
    "<br><br><br>\n",
    "二、从更高的层次看待伯努利分布\n",
    "<br><br><br>\n",
    "那么，如何根据x求出y所属的伯努利分布的参数Φ呢。\n",
    "<br><br>\n",
    "直接看，似乎没什么思路，我们需要换个角度。\n",
    "<br><br>\n",
    "伯努利分布实际上属于某一大类分布中的一种情况。这一大类分布就是指数分布族。\n",
    "这就好比， x + 1=0是一个方程，但从更广泛的角度来看，它只是 ax + b = 0一次方程的一种具体情况而已。\n",
    "<br><br>\n",
    "从指数分布族的角度来分析，我们很容易构建起x与伯努利分布参数的联系。\n",
    "<br><br><br>\n",
    "三、指数分布族\n",
    "<br><br><br>\n",
    "下面，我们就来看看指数分布族是什么样子![指数分布族](https://upload-images.jianshu.io/upload_images/1371984-de080554834ced2e.png)\n",
    "\n",
    "为了简化理解，你可以自动忽略η上面的大写字母T，仅仅将η作为一个实数来理解。\n",
    "<br><br>\n",
    "<br>它其实是在告诉我们：\n",
    "<br>对于一个随机变量x，只要你确定三个函数，就可以确定一类分布。\n",
    "<br>这三个函数就是：\n",
    "<br><br>\n",
    "<br>h(x)\n",
    "<br>T(x)\n",
    "<br>A(η)\n",
    "<br>η用来确定该类分布的具体参数。\n",
    "<br>对于我们的伯努利分布，这三个函数是什么样子呢？我们可以从伯努利分布出发，一路变形到与指数分布族一样的形式。如下所示：![](https://upload-images.jianshu.io/upload_images/1371984-decea5ecdc734f2a.jpg)  \n",
    "请认真看看上面的变形推导过程。可以看到，伯努利分布确实可以改写成指数分布族的形式。并且，伯努利分布的参数Φ与η之间，还有一个sigmoid的函数关系。\n",
    "<br><br><br>\n",
    "4、最后一步<br><br>\n",
    "<br>现在，我们看到，伯努利分布确实是指数分布族的一个特殊情况，它的参数Φ与指数分布族中的参数η还有对应关系。\n",
    "<br>这意味着，我们如果能找到x和η之间的关系，那么也就找到了x和Φ之间的关系。\n",
    "<br>\n",
    "<br>在这里，我们需要再做一个假设，那就是\n",
    "<br>η和x之间存在线性的关系！！注意，这是我们做的第二个假设哦。即：\n",
    "<br>η = θx。\n",
    "<br>\n",
    "<br>有了这个假设，我们的模型训练过程就是这样的：\n",
    "<br>\n",
    "<br>对一个x，根据 θx算出η\n",
    "<br>根据η算出Φ\n",
    "<br>因为Φ既是伯努利分布的唯一参数，也是该分布的期望，所以将Φ作为预测值。\n",
    "<br>计算Φ与真实的标签y之间的误差loss。（通常用交叉熵）\n",
    "<br>通过SGD来更新θ，降低loss。\n",
    "<br>这不就是我们的逻辑回归么？\n",
    "<br><br>\n",
    "<br>5总结<br><br>\n",
    "<br>可见，逻辑回归模型之所以是sigmoid 的形式，源于我们假设y服从伯努利分布，伯努利分布又属于指数分布族，经过推导，将伯努利分布变<br>成指数分布族的形式后。我们发现伯努利分布的唯一参数Φ与指数分布族中的参数η具有sigmoid函数关系，于是我们转而求η与x的关系，<br>此时，我们又假设η与x具有线性关系。\n",
    "<br>至此，找到了我们要用的模型的样子，也就是逻辑回归。\n",
    "<br>\n",
    "<br>回答文章开头的问题，逻辑回归输出的到底是不是概率呢？答案是如果你的情况满足本文所说的两个假设，那么你训练模型的过程，就确实是在对概率进行建模。\n",
    "<br>\n",
    "<br>这两个假设并不是那么容易满足的。所以，很多情况下，我们得出的逻辑回归输出值，无法当作真实的概率，只能作为置信度来使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
