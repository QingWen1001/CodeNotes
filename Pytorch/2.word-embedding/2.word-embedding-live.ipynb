{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量\n",
    "- 学习词向量的概念\n",
    "- 使用 skip-thought 模型训练词向量\n",
    "- 使用 pytorch dataset 和 dataloader\n",
    "- 学习定义 pytorch 模型\n",
    "- 学习torch。nn 中的 Module\n",
    "  -  Embedding\n",
    "- 学习常见的 pytorch 的 operations\n",
    "  - bmm\n",
    "  - logsigmoid\n",
    "- 保存和读取 pytorch 模型\n",
    "\n",
    "第二课使用的训练数据可以从以下链接下载到。\n",
    "\n",
    "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg 密码:v2z5\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文Distributed Representations of Words and Phrases and their Compositionality中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "\n",
    "subsampling：参考论文section 2.3\n",
    "\n",
    "**两个模块的区别：**[torch.nn 和 torch.functional 的区别](https://blog.csdn.net/hawkcici160/article/details/80140059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity # 余弦相似度\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "#为了方式 随机数对数据结果产生的影响，这里设置所有的 random.seed 是 1\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA :\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "\n",
    "# set some hyper parameters\n",
    "C = 3 # context window\n",
    "K = 100 # number of negative samples\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_DECENT = 150\n",
    "VOCAB_SIZE = 30000 # the size of vocab ,not all word in the vocab\n",
    "\n",
    "LOG_FILE = \"word-embedding.log\"\n",
    "\n",
    "def word_tokenize(text):\n",
    "    ''' split the sequence as words '''\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('text8.train.txt','r')as file:\n",
    "    text = file.read()\n",
    "    #text = text.split()\n",
    "text = [w for w in word_tokenize(text)]\n",
    "\n",
    "''' 使用最常见的单词制作词典实际上是不合适的，因为文本未经过处理，一些意义不大的常用词可能需要剔除 ，这一步需要事先对 text 进行处理 '''\n",
    "vocab = dict (Counter(text).most_common(VOCAB_SIZE-1)) # 这里 -1 是为了给不常用词留一个位置 \n",
    "vocab['<UNK>'] = len(text) - np.sum(list(vocab.values())) # vocab.values() 获取所有词的出现的次数 \n",
    "\n",
    "id2word = [w for w in vocab.keys()]\n",
    "word2id = {w: i for i,w in enumerate(id2word)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "word_counts =  np.array([count for count in vocab.values()],dtype = np.float32)\n",
    "word_freq = word_counts / np.sum(word_counts)\n",
    "# 对词频进行 3/4 次方处理\n",
    "word_freq = word_freq ** (0.75)\n",
    "word_freq = word_freq / np.sum(word_freq)\n",
    "print(word_freq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word2id, id2word, word_freq, word_counts):\n",
    "        ''' 初始化 '''\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word_to_idx: the dictionary from word to idx\n",
    "            idx_to_word: idx to word mapping\n",
    "            word_freq: the frequency of each word\n",
    "            word_counts: the word counts\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__()\n",
    "        \n",
    "        # 语料 数字化  后面的 word2id['UNK'] 就是 当单词不在常用词列表里时 用 UNK 的 id 代替\n",
    "        self.text_encoded = [word2id.get(word, word2id['<UNK>']) for word in text] \n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long() # 将 text_encoded 转换成 long tensor 类型\n",
    "        \n",
    "        self.word2id = word2id\n",
    "        self.id2word = id2word\n",
    "        self.word_freq = torch.Tensor(word_freq)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "    def __len__(self,):\n",
    "        ''' 数据集中 item 的数量 '''\n",
    "        return len(self.text_encoded)\n",
    "    def __getitem__(self,idx):\n",
    "        ''' 根据 index 返回对应的 item'''\n",
    "        center_word = self.text_encoded[idx] # 中心词\n",
    "        \n",
    "        # 获取周围词\n",
    "        pos_indices = list(range(idx - C))+list(range(idx+1,idx+C+1)) #获取周围词列表 range 不会取到小于 0 的位置\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 对于 idx + word_window +1 大于 text 的长度时特殊处理\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        '''\n",
    "        \n",
    "        # 获取周围词\n",
    "        start = max(0,idx-C)\n",
    "        end = min(len(self.text_encoded),(idx+1+C))\n",
    "        pos_wrods = self.text_encoded[start:idx]+self.text_encoded[idx+1,end]\n",
    "        '''\n",
    "        \n",
    "        ''' multinomial 函数的参数： 元素分布的概率， 需要采样的个数， 是否可以重复采样同一个单词\n",
    "            self.word_freq：单词处理后的概率， \n",
    "            K * pos_words.shape[0]：每个正例单词需要采样 k 个负例单词\n",
    "            replacement = True ： 是否可以重复采样同一个单词\n",
    "        '''\n",
    "        neg_words = torch.multinomial(self.word_freq, K * pos_words.shape[0], True)\n",
    "        \n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建 dataset 和 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset( text, word2id, id2word, word_freq, word_counts)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)      # shuffle :打乱顺序。 num_workers ：线程数量，多线程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader理解：https://blog.csdn.net/qq_36653505/article/details/83351808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 PyTorch 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels:[batch_size]\n",
    "        pos_labels:[batch_size,(window_size *2)]\n",
    "        neg_labels:[batch_size,(window_size *2 *k)]\n",
    "        '''\n",
    "        input_embedding = self.in_embed(input_labels) #[batch_size, embed_size]\n",
    "        input_embedding = input_embedding.unsqueeze(2) # 创建一个 1 的维度 [batch_size, embed_size，1]\n",
    "        \n",
    "        pos_embedding = self.in_embed(pos_labels) #[batch_size, window_size *2, embed_size]\n",
    "        neg_embedding = self.in_embed(neg_labels) #[batch_size, window_size *2 *k, embed_size]\n",
    "        \n",
    "        # bmm :batch matrix multiply 维度 [b,c,d] * [b,d,e] = [b,c,e]\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding).sequeeze(2)#[batch_size, window_size *2, 1].squeeze ->[batch_size, window_size *2]\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding).sequeeze(2) #[batch_size, window_size *2 *k]\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "        \n",
    "    def input_embedding(self):\n",
    "        ''' 获取 input embedding'''\n",
    "        return self.in_embed.wight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个模型以及把模型移动到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_DECENT)\n",
    "#得到model，有参数，有loss，可以优化了\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是评估模型的代码，以及训练模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename, embedding_weights): \n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        print(input_labels, pos_labels, neg_labels)\n",
    "        if i>5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean() #求平均\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #打印结果。\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "            \n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            embedding_weights = model.input_embeddings()\n",
    "            sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "            sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "            sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                \n",
    "    embedding_weights = model.input_embeddings()\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 MEN 和 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
    "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
    "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_idx = word_to_idx[\"man\"] \n",
    "king_idx = word_to_idx[\"king\"] \n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a=[0,1,2,3,4,5,6,7,8,9]\n",
    "b = a[:3]+a[4:6]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
