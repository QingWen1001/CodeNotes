{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量\n",
    "- 学习词向量的概念\n",
    "- 使用 skip-thought 模型训练词向量\n",
    "- 使用 pytorch dataset 和 dataloader\n",
    "- 学习定义 pytorch 模型\n",
    "- 学习torch。nn 中的 Module\n",
    "  -  Embedding\n",
    "- 学习常见的 pytorch 的 operations\n",
    "  - bmm\n",
    "  - logsigmoid\n",
    "- 保存和读取 pytorch 模型\n",
    "\n",
    "第二课使用的训练数据可以从以下链接下载到。\n",
    "\n",
    "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg 密码:v2z5\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文Distributed Representations of Words and Phrases and their Compositionality中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "\n",
    "subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity # 余弦相似度\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "#为了方式 随机数对数据结果产生的影响，这里设置所有的 random.seed 是 1\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA :\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "\n",
    "# set some hyper parameters\n",
    "word_window = 3 # context window\n",
    "samples_size = 100 # number of negative samples\n",
    "NUM_EPOCH = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_DECENT = 150\n",
    "VOCAB_SIZE = 30000 # the size of vocab ,not all word in the vocab\n",
    "\n",
    "def word_tokenize(text):\n",
    "    ''' split the sequence as words '''\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('text8.train.txt','r')as file:\n",
    "    text = file.read()\n",
    "    #text = text.split()\n",
    "text = [w for w in word_tokenize(text)]\n",
    "\n",
    "''' 使用最常见的单词制作词典实际上是不合适的，因为文本未经过处理，一些意义不大的常用词可能需要剔除 ，这一步需要事先对 text 进行处理 '''\n",
    "vocab = dict (Counter(text).most_common(VOCAB_SIZE-1)) # 这里 -1 是为了给不常用词留一个位置 \n",
    "vocab['<UNK>'] = len(text) - np.sum(list(vocab.values())) # vocab.values() 获取所有词的出现的次数 \n",
    "vocab\n",
    "id2word = [w for w in vocab.keys()]\n",
    "word2id = {w: i for i,w in enumerate(id2word)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "word_counts =  np.array([count for count in vocab.values()],dtype = np.float32)\n",
    "word_freq = word_counts / np.sum(word_counts)\n",
    "# 对词频进行 3/4 次方处理\n",
    "word_freq = word_freq ** (0.75)\n",
    "word_freq = word_counts / np.sum(word_counts)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
